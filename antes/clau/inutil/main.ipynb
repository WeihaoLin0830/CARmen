{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a58b04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sam2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msegment_anything_2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2_image_predictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2ImagePredictor  \u001b[38;5;66;03m# Importación corregida\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub\\segment_anything_2\\sam2\\sam2_image_predictor.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msam2_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2Base\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msam2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAM2Transforms\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSAM2ImagePredictor\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sam2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Añade la ruta del proyecto al path de Python\n",
    "sys.path.append(r\"C:\\Users\\Claudia\\Documents\\GitHub\")  # <-- Ruta crítica\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from segment_anything_2.sam2.sam2_image_predictor import SAM2ImagePredictor  # Importación corregida\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure required dependencies are installed\n",
    "%pip install hydra-core\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a90610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga de API keys desde .env ---\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "# --- Configuración de dispositivos ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Modelos para segmentación y caption ---\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\n",
    "    \"facebook/sam2-hiera-large\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "processor_blip = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model_blip     = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "# --- Modelos para RAG+CLIP ---\n",
    "# CLIP para embeddings de crop\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# MiniLM para embeddings de texto\n",
    "model_text     = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Cliente y colecciones Chroma ---\n",
    "client    = chromadb.PersistentClient(path=\"chroma\")\n",
    "text_col  = client.get_or_create_collection(\n",
    "    name=\"tavascan_text\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "img_col   = client.get_or_create_collection(\n",
    "    name=\"tavascan_images\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# --- Lectura de imagen ---\n",
    "img_bgr = cv2.imread(\"C:/Users/weiha/Documents/HackUPC_2025/img/dashboard.jpeg\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9d6e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenadas en imagen original: (483,107) → (672,186)\n"
     ]
    }
   ],
   "source": [
    "# Variables globales\n",
    "ref_pt = []           # puntos de la selección (en coords de display)\n",
    "drawing = False       # flag de arrastre\n",
    "scale_percent = 100   # zoom inicial en porcentaje\n",
    "img = img_bgr.copy()  # imagen original\n",
    "h_orig, w_orig = img.shape[:2]\n",
    "\n",
    "# Función para recalcular la imagen mostrada según el zoom\n",
    "def update_display():\n",
    "    global img_display, img_copy, scale\n",
    "    scale = scale_percent / 100.0\n",
    "    w_disp = int(w_orig * scale)\n",
    "    h_disp = int(h_orig * scale)\n",
    "    img_display = cv2.resize(img, (w_disp, h_disp), interpolation=cv2.INTER_AREA)\n",
    "    img_copy = img_display.copy()\n",
    "    # Si ya hay ref_pt, redibuja el rectángulo en img_copy\n",
    "    if len(ref_pt) == 2:\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "\n",
    "# Callback para el trackbar de zoom\n",
    "def on_trackbar(val):\n",
    "    global scale_percent\n",
    "    scale_percent = max(val, 1)  # evita 0%\n",
    "    update_display()\n",
    "\n",
    "# Callback para el mouse: dibuja y captura la caja en coords de display\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global ref_pt, drawing, img_copy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ref_pt = [(x, y)]\n",
    "    elif event == cv2.EVENT_MOUSEMOVE and drawing:\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], (x, y), (0, 255, 0), 2)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        ref_pt.append((x, y))\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "        # Mapea coords de display a coords originales\n",
    "        x0_disp, y0_disp = ref_pt[0]\n",
    "        x1_disp, y1_disp = ref_pt[1]\n",
    "        x0 = int(x0_disp / scale)\n",
    "        y0 = int(y0_disp / scale)\n",
    "        x1 = int(x1_disp / scale)\n",
    "        y1 = int(y1_disp / scale)\n",
    "        print(f\"Coordenadas en imagen original: ({x0},{y0}) → ({x1},{y1})\")\n",
    "\n",
    "# Prepara la ventana y los callbacks\n",
    "update_display()\n",
    "cv2.namedWindow(\"Selector de caja\")\n",
    "cv2.createTrackbar(\"Zoom %\", \"Selector de caja\", scale_percent, 300, on_trackbar)\n",
    "cv2.setMouseCallback(\"Selector de caja\", mouse_callback)\n",
    "\n",
    "# Bucle de visualización\n",
    "while True:\n",
    "    cv2.imshow(\"Selector de caja\", img_copy)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"r\"):       # 'r' resetea la selección\n",
    "        ref_pt = []\n",
    "        img_copy = img_display.copy()\n",
    "    elif key == ord(\"q\"):     # 'q' sale\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b362ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tras el bucle y antes de usar predictor:\n",
    "# x0_disp, y0_disp = ref_pt[0]\n",
    "# x1_disp, y1_disp = ref_pt[1]\n",
    "\n",
    "# # Asegura que (x0_disp,y0_disp) sea la esquina superior-izquierda\n",
    "# x0_disp, x1_disp = sorted([x0_disp, x1_disp])\n",
    "# y0_disp, y1_disp = sorted([y0_disp, y1_disp])\n",
    "\n",
    "# # Mapea a coordenadas de la imagen original y clampa\n",
    "# x0 = int(x0_disp / scale)\n",
    "# y0 = int(y0_disp / scale)\n",
    "# x1 = int(x1_disp / scale)\n",
    "# y1 = int(y1_disp / scale)\n",
    "\n",
    "# # Evita salirse de los límites\n",
    "# x0 = max(0, min(x0, w_orig - 1))\n",
    "# y0 = max(0, min(y0, h_orig - 1))\n",
    "# x1 = max(1, min(x1, w_orig    ))\n",
    "# y1 = max(1, min(y1, h_orig    ))\n",
    "\n",
    "boxleft = 492\n",
    "boxtop = 116\n",
    "boxwidth = 150\n",
    "boxheight = 69\n",
    "\n",
    "x0 = boxleft\n",
    "y0 = boxtop\n",
    "x1 = boxleft + boxwidth\n",
    "y1 = boxtop + boxheight\n",
    "\n",
    "box = np.array([[x0, y0, x1, y1]], dtype=np.int32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=box,\n",
    "        multimask_output=False,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "mask = masks[0]\n",
    "crop = img_rgb[y0:y1, x0:x1]\n",
    "\n",
    "# convierte de RGB a BGR para cv2\n",
    "crop_bgr = cv2.cvtColor(crop, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow(\"Region seleccionada\", crop_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d49e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta RAG+CLIP:\n",
      " parts {\n",
      "  text: \"Sin información adicional disponible, la imagen muestra el Digital Cockpit del CUPRA Tavascan. En específico, se puede ver la velocidad del vehículo (120) y otros indicadores del sistema. Sin embargo, sin más contexto o una selección específica en la imagen, es difícil dar una respuesta más precisa.\\n\"\n",
      "}\n",
      "role: \"model\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- RAG+CLIP retrieval ---\n",
    "# 1) Embedding de la región con CLIP\n",
    "inputs_img = clip_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    img_emb = clip_model.get_image_features(**inputs_img)\n",
    "    img_emb = img_emb / img_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "img_emb = img_emb.cpu().numpy()[0]\n",
    "# 2) Query en colecciones\n",
    "txt_hits = text_col.query(query_embeddings=[img_emb], n_results=4)\n",
    "img_hits = img_col.query(query_embeddings=[img_emb], n_results=2)\n",
    "# 3) Construir contexto\n",
    "context_txt = \"\\n\".join(\n",
    "    f\"- {doc} (p.{meta['page']}, col.{meta['column']})\"\n",
    "    for doc, meta in zip(txt_hits[\"documents\"][0], txt_hits[\"metadatas\"][0])\n",
    ")\n",
    "context_img = \"\\n\".join(img_hits[\"documents\"][0])\n",
    "\n",
    "# 4) Prompt RAG\n",
    "prompt = (\n",
    "    \"Eres un experto en el manual del CUPRA Tavascan.\\n\"\n",
    "    \"Información relevante del manual:\\n\" + context_txt + \"\\n\"\n",
    "    \"Imágenes relacionadas:\\n\" + context_img + \"\\n\"\n",
    "    \"¿Qué es la parte seleccionada en la imagen?\"\n",
    ")\n",
    "\n",
    "# 5) Llamada a Gemini incluyendo la imagen\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
    "\n",
    "# Convertir el crop a formato adecuado para Gemini\n",
    "crop_pil = Image.fromarray(crop)  # Convertir de numpy array a PIL Image\n",
    "\n",
    "# Crear consulta multimodal con texto e imagen\n",
    "response = model.generate_content([prompt, crop_pil])\n",
    "print(\"Respuesta RAG+CLIP:\\n\", response.candidates[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
