{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a58b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from openai import ChatCompletion\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4a90610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración de dispositivos ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Modelos para segmentación y caption ---\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\n",
    "    \"facebook/sam2-hiera-large\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "processor_blip = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model_blip     = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "# --- Modelos para RAG+CLIP ---\n",
    "# CLIP para embeddings de crop\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# MiniLM para embeddings de texto\n",
    "model_text     = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Cliente y colecciones Chroma ---\n",
    "client    = chromadb.PersistentClient(path=\"chroma\")\n",
    "text_col  = client.get_or_create_collection(\n",
    "    name=\"tavascan_text\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "img_col   = client.get_or_create_collection(\n",
    "    name=\"tavascan_images\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# --- Lectura de imagen ---\n",
    "img_bgr = cv2.imread(\"C:/Users/weiha/Documents/HackUPC_2025/img/dashboard.jpeg\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d9d6e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenadas en imagen original: (305,68) → (697,427)\n"
     ]
    }
   ],
   "source": [
    "# Variables globales\n",
    "ref_pt = []           # puntos de la selección (en coords de display)\n",
    "drawing = False       # flag de arrastre\n",
    "scale_percent = 100   # zoom inicial en porcentaje\n",
    "img = img_bgr.copy()  # imagen original\n",
    "h_orig, w_orig = img.shape[:2]\n",
    "\n",
    "# Función para recalcular la imagen mostrada según el zoom\n",
    "def update_display():\n",
    "    global img_display, img_copy, scale\n",
    "    scale = scale_percent / 100.0\n",
    "    w_disp = int(w_orig * scale)\n",
    "    h_disp = int(h_orig * scale)\n",
    "    img_display = cv2.resize(img, (w_disp, h_disp), interpolation=cv2.INTER_AREA)\n",
    "    img_copy = img_display.copy()\n",
    "    # Si ya hay ref_pt, redibuja el rectángulo en img_copy\n",
    "    if len(ref_pt) == 2:\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "\n",
    "# Callback para el trackbar de zoom\n",
    "def on_trackbar(val):\n",
    "    global scale_percent\n",
    "    scale_percent = max(val, 1)  # evita 0%\n",
    "    update_display()\n",
    "\n",
    "# Callback para el mouse: dibuja y captura la caja en coords de display\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global ref_pt, drawing, img_copy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ref_pt = [(x, y)]\n",
    "    elif event == cv2.EVENT_MOUSEMOVE and drawing:\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], (x, y), (0, 255, 0), 2)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        ref_pt.append((x, y))\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "        # Mapea coords de display a coords originales\n",
    "        x0_disp, y0_disp = ref_pt[0]\n",
    "        x1_disp, y1_disp = ref_pt[1]\n",
    "        x0 = int(x0_disp / scale)\n",
    "        y0 = int(y0_disp / scale)\n",
    "        x1 = int(x1_disp / scale)\n",
    "        y1 = int(y1_disp / scale)\n",
    "        print(f\"Coordenadas en imagen original: ({x0},{y0}) → ({x1},{y1})\")\n",
    "\n",
    "# Prepara la ventana y los callbacks\n",
    "update_display()\n",
    "cv2.namedWindow(\"Selector de caja\")\n",
    "cv2.createTrackbar(\"Zoom %\", \"Selector de caja\", scale_percent, 300, on_trackbar)\n",
    "cv2.setMouseCallback(\"Selector de caja\", mouse_callback)\n",
    "\n",
    "# Bucle de visualización\n",
    "while True:\n",
    "    cv2.imshow(\"Selector de caja\", img_copy)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"r\"):       # 'r' resetea la selección\n",
    "        ref_pt = []\n",
    "        img_copy = img_display.copy()\n",
    "    elif key == ord(\"q\"):     # 'q' sale\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b362ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tras el bucle y antes de usar predictor:\n",
    "x0_disp, y0_disp = ref_pt[0]\n",
    "x1_disp, y1_disp = ref_pt[1]\n",
    "\n",
    "# Asegura que (x0_disp,y0_disp) sea la esquina superior-izquierda\n",
    "x0_disp, x1_disp = sorted([x0_disp, x1_disp])\n",
    "y0_disp, y1_disp = sorted([y0_disp, y1_disp])\n",
    "\n",
    "# Mapea a coordenadas de la imagen original y clampa\n",
    "x0 = int(x0_disp / scale)\n",
    "y0 = int(y0_disp / scale)\n",
    "x1 = int(x1_disp / scale)\n",
    "y1 = int(y1_disp / scale)\n",
    "\n",
    "# Evita salirse de los límites\n",
    "x0 = max(0, min(x0, w_orig - 1))\n",
    "y0 = max(0, min(y0, h_orig - 1))\n",
    "x1 = max(1, min(x1, w_orig    ))\n",
    "y1 = max(1, min(y1, h_orig    ))\n",
    "\n",
    "box = np.array([[x0, y0, x1, y1]], dtype=np.int32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=box,\n",
    "        multimask_output=False,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "mask = masks[0]\n",
    "crop = img_rgb[y0:y1, x0:x1]\n",
    "\n",
    "# convierte de RGB a BGR para cv2\n",
    "crop_bgr = cv2.cvtColor(crop, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow(\"Region seleccionada\", crop_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "28d49e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEres un experto en CUPRA Tavascan.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mContexto:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext_txt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImágenes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcontext_img\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m¿Qué es la parte seleccionada?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# LLM call\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m ChatCompletion\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m resp \u001b[38;5;241m=\u001b[39m ChatCompletion\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prompt}]\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRespuesta:\u001b[39m\u001b[38;5;124m\"\u001b[39m, resp\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# --- RAG+CLIP retrieval ---\n",
    "# Embedding imagen\n",
    "inputs_img = clip_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    img_emb = clip_model.get_image_features(**inputs_img)\n",
    "    img_emb = img_emb / img_emb.norm(p=2)\n",
    "img_emb = img_emb.cpu().numpy()\n",
    "# Recuperar\n",
    "txt_hits = text_col.query(query_embeddings=[img_emb[0]], n_results=4)\n",
    "img_hits = img_col.query(query_embeddings=[img_emb[0]], n_results=2)\n",
    "# Construir prompt\n",
    "context_txt = \"\\n\".join(f\"- {d} (p.{m['page']},col.{m['column']})\" for d,m in zip(txt_hits['documents'][0], txt_hits['metadatas'][0]))\n",
    "context_img = \"\\n\".join(img_hits['documents'][0])\n",
    "prompt = f\"Eres un experto en CUPRA Tavascan.\\nContexto:\\n{context_txt}\\nImágenes:\\n{context_img}\\n¿Qué es la parte seleccionada?\"\n",
    "# LLM call\n",
    "ChatCompletion.api_key = os.getenv('OPENAI_API_KEY')\n",
    "resp = ChatCompletion.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt}]\n",
    ")\n",
    "print(\"Respuesta:\", resp.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
