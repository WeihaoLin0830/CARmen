{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a58b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a90610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga de API keys desde .env ---\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "# --- Configuración de dispositivos ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Modelo para segmentación y caption ---\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\n",
    "    \"facebook/sam2-hiera-large\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# --- Modelos para RAG+CLIP ---\n",
    "# CLIP para embeddings de crop\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# MiniLM para embeddings de texto\n",
    "model_text     = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Cliente y colecciones Chroma ---\n",
    "client    = chromadb.PersistentClient(path=\"C:/Users/weiha/Documents/HackUPC_2025/extracted_content_manual/chroma_db\")\n",
    "text_col  = client.get_or_create_collection(\n",
    "    name=\"tavascan_text\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "img_col   = client.get_or_create_collection(\n",
    "    name=\"tavascan_images\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# --- Lectura de imagen ---\n",
    "img_bgr = cv2.imread(\"C:/Users/weiha/Documents/HackUPC_2025/img/dashboard.jpeg\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d6e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenadas en imagen original: (343,94) → (678,409)\n"
     ]
    }
   ],
   "source": [
    "# Variables globales\n",
    "ref_pt = []           # puntos de la selección (en coords de display)\n",
    "drawing = False       # flag de arrastre\n",
    "scale_percent = 100   # zoom inicial en porcentaje\n",
    "img = img_bgr.copy()  # imagen original\n",
    "h_orig, w_orig = img.shape[:2]\n",
    "\n",
    "# Función para recalcular la imagen mostrada según el zoom\n",
    "def update_display():\n",
    "    global img_display, img_copy, scale\n",
    "    scale = scale_percent / 100.0\n",
    "    w_disp = int(w_orig * scale)\n",
    "    h_disp = int(h_orig * scale)\n",
    "    img_display = cv2.resize(img, (w_disp, h_disp), interpolation=cv2.INTER_AREA)\n",
    "    img_copy = img_display.copy()\n",
    "    # Si ya hay ref_pt, redibuja el rectángulo en img_copy\n",
    "    if len(ref_pt) == 2:\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "\n",
    "# Callback para el trackbar de zoom\n",
    "def on_trackbar(val):\n",
    "    global scale_percent\n",
    "    scale_percent = max(val, 1)  # evita 0%\n",
    "    update_display()\n",
    "\n",
    "# Callback para el mouse: dibuja y captura la caja en coords de display\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global ref_pt, drawing, img_copy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ref_pt = [(x, y)]\n",
    "    elif event == cv2.EVENT_MOUSEMOVE and drawing:\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], (x, y), (0, 255, 0), 2)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        ref_pt.append((x, y))\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "        # Mapea coords de display a coords originales\n",
    "        x0_disp, y0_disp = ref_pt[0]\n",
    "        x1_disp, y1_disp = ref_pt[1]\n",
    "        x0 = int(x0_disp / scale)\n",
    "        y0 = int(y0_disp / scale)\n",
    "        x1 = int(x1_disp / scale)\n",
    "        y1 = int(y1_disp / scale)\n",
    "        print(f\"Coordenadas en imagen original: ({x0},{y0}) → ({x1},{y1})\")\n",
    "\n",
    "# Prepara la ventana y los callbacks\n",
    "update_display()\n",
    "cv2.namedWindow(\"Selector de caja\")\n",
    "cv2.createTrackbar(\"Zoom %\", \"Selector de caja\", scale_percent, 300, on_trackbar)\n",
    "cv2.setMouseCallback(\"Selector de caja\", mouse_callback)\n",
    "\n",
    "# Bucle de visualización\n",
    "while True:\n",
    "    cv2.imshow(\"Selector de caja\", img_copy)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"r\"):       # 'r' resetea la selección\n",
    "        ref_pt = []\n",
    "        img_copy = img_display.copy()\n",
    "    elif key == ord(\"q\"):     # 'q' sale\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b362ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tras el bucle y antes de usar predictor:\n",
    "x0_disp, y0_disp = ref_pt[0]\n",
    "x1_disp, y1_disp = ref_pt[1]\n",
    "\n",
    "# Asegura que (x0_disp,y0_disp) sea la esquina superior-izquierda\n",
    "x0_disp, x1_disp = sorted([x0_disp, x1_disp])\n",
    "y0_disp, y1_disp = sorted([y0_disp, y1_disp])\n",
    "\n",
    "# Mapea a coordenadas de la imagen original y clampa\n",
    "x0 = int(x0_disp / scale)\n",
    "y0 = int(y0_disp / scale)\n",
    "x1 = int(x1_disp / scale)\n",
    "y1 = int(y1_disp / scale)\n",
    "\n",
    "# Evita salirse de los límites\n",
    "x0 = max(0, min(x0, w_orig - 1))\n",
    "y0 = max(0, min(y0, h_orig - 1))\n",
    "x1 = max(1, min(x1, w_orig    ))\n",
    "y1 = max(1, min(y1, h_orig    ))\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# boxleft = 492\n",
    "# boxtop = 116\n",
    "# boxwidth = 150\n",
    "# boxheight = 69\n",
    "\n",
    "# x0 = boxleft\n",
    "# y0 = boxtop\n",
    "# x1 = boxleft + boxwidth\n",
    "# y1 = boxtop + boxheight\n",
    "\n",
    "box = np.array([[x0, y0, x1, y1]], dtype=np.int32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=box,\n",
    "        multimask_output=False,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "mask = masks[0]\n",
    "crop = img_rgb[y0:y1, x0:x1]\n",
    "\n",
    "# convierte de RGB a BGR para cv2\n",
    "crop_bgr = cv2.cvtColor(crop, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow(\"Region seleccionada\", crop_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68990162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated description: The image shows the CUPRA Tavascan's multifunction steering wheel with capacitive touch controls. It features an embedded driver information display, showing critical vehicle data. It also features paddles for regeneration adjustment.\n",
      "Extracted keywords: Here are 4 technical key terms extracted from the text that would likely appear in the official CUPRA Tavascan manual:\n",
      "\n",
      "1.  **Multifunction Steering Wheel:** This describes a steering wheel with multiple integrated controls.\n",
      "2.  **Capacitive Touch Controls:** Refers to controls that respond to the touch of a finger, relying on capacitive sensing.\n",
      "3.  **Driver Information Display:** This refers to the screen displaying essential data to the driver.\n",
      "4.  **Regeneration Adjustment:** Pertains to the system that recovers energy during deceleration, often adjustable in electric vehicles.\n",
      "\n",
      "Enhanced RAG response:\n",
      " The image shows the CUPRA Tavascan's **Multifunction Steering Wheel**. This steering wheel incorporates modules allowing the driver to control audio, telephone, navigation, voice control, and assist functions without being distracted from the road.\n",
      "\n",
      "The steering wheel includes **Capacitive Touch Controls**. Specifically, the image shows controls for volume adjustment (volume up/down via press or swipe) and for activating or pausing the ACC/Travel Assist/Speed Limiter functions. These controls respond to the touch of a finger, relying on capacitive sensing. The driver can also use these **Capacitive Touch Controls** to modify the programmed ACC distance or to increase or decrease the programmed speed, as well as to control the HUD (Head-up-Display).\n",
      "\n",
      "Embedded within the dashboard and visible through the steering wheel is the **Driver Information Display**. This screen displays essential vehicle data to the driver. It is a digital instrument cluster with a high-resolution color liquid crystal display. The **Driver Information Display** is also referred to as the \"digital instrument cluster\". In addition to the speedometer, by selecting different information profiles you can display information from the driver assistant systems. The **Driver Information Display** can display views, such as:\n",
      "*   Basic: Driving indications with information on driver assistant systems, speed and navigation.\n",
      "*   Driver assistant systems: Display of active driver assistant systems and speed. The navigation context is hidden.\n",
      "*   Navigation: Representation with information about the guided route and speed. The graphic view of the driver assistant systems is hidden.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG con descripción textual como puente (VERSIÓN CORREGIDA) ---\n",
    "# 1) Generate image description with Gemini\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
    "crop_pil = Image.fromarray(crop)\n",
    "\n",
    "# Request a detailed description of the selected region\n",
    "description_prompt = \"Describe briefly and technically what component of the CUPRA Tavascan dashboard is shown in this image. Be specific and use technical terms.\"\n",
    "description_response = model.generate_content([description_prompt, crop_pil])\n",
    "image_description = description_response.text.strip()  # <-- CORRECTED\n",
    "\n",
    "print(f\"Generated description: {image_description}\")\n",
    "\n",
    "# 2) Generate text embedding from the description\n",
    "text_embedding = model_text.encode([image_description], normalize_embeddings=True)[0]\n",
    "\n",
    "# 3) Query collections using text embedding\n",
    "txt_hits = text_col.query(\n",
    "    query_embeddings=[text_embedding.tolist()],\n",
    "    n_results=4,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "# 4) Extract keywords to enrich the context\n",
    "keywords_prompt = \"Extract 3-5 technical key terms from this text that appear in the official CUPRA Tavascan manual: \" + image_description\n",
    "keywords_response = model.generate_content(keywords_prompt)\n",
    "keywords = keywords_response.text.strip()  # <-- CORRECTED\n",
    "\n",
    "print(f\"Extracted keywords: {keywords}\")\n",
    "\n",
    "# 5) Build context with retrieved information\n",
    "if txt_hits[\"documents\"] and len(txt_hits[\"documents\"][0]) > 0:\n",
    "    context_txt = \"\\n\".join(\n",
    "        f\"- {doc} (p.{meta.get('start_page', 'N/A')}, relevance: {score:.2f})\"\n",
    "        for doc, meta, score in zip(txt_hits[\"documents\"][0], txt_hits[\"metadatas\"][0], txt_hits[\"distances\"][0])\n",
    "    )\n",
    "else:\n",
    "    context_txt = \"No relevant fragments found in the manual.\"\n",
    "\n",
    "# 6) Complete RAG prompt\n",
    "prompt = (\n",
    "    f\"You are an expert on the CUPRA Tavascan owner's manual.\\n\\n\"\n",
    "    f\"The image shows: {image_description}\\n\\n\"\n",
    "    f\"Key terms: {keywords}\\n\\n\"\n",
    "    f\"Relevant information from the manual:\\n{context_txt}\\n\\n\"\n",
    "    f\"Based on this information and what you observe in the image, explain in detail \"\n",
    "    f\"what component of the CUPRA Tavascan dashboard is shown, what it is used for, \"\n",
    "    f\"and how it should be used. Use only official terminology that appears in the manual.\"\n",
    ")\n",
    "\n",
    "# 7) Create multimodal query with text and image\n",
    "response = model.generate_content([prompt, crop_pil])\n",
    "print(\"\\nEnhanced RAG response:\\n\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ranking de imágenes por similitud visual ---\n",
    "# Directorio de imágenes del manual\n",
    "images_folder = \"C:/Users/weiha/Documents/HackUPC_2025/extracted_content_manual/images\"\n",
    "\n",
    "def rank_similar_images(input_image, top_k=5):\n",
    "    \"\"\"Rank manual images by similarity to input image\"\"\"\n",
    "    # Si input_image es un array (ej: crop), convertir a PIL Image\n",
    "    if isinstance(input_image, np.ndarray):\n",
    "        input_pil = Image.fromarray(input_image)\n",
    "        # Guardar temporalmente\n",
    "        temp_path = \"temp_query_image.jpg\"\n",
    "        input_pil.save(temp_path)\n",
    "        query_path = temp_path\n",
    "    else:\n",
    "        # Si ya es una ruta\n",
    "        query_path = input_image\n",
    "\n",
    "    # Obtener embedding de la imagen query con el mismo modelo CLIP ya cargado\n",
    "    inputs_query = clip_processor(images=Image.open(query_path).convert(\"RGB\"), return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        query_emb = clip_model.get_image_features(**inputs_query)\n",
    "        query_emb = query_emb / query_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    query_emb = query_emb.cpu().numpy()[0]\n",
    "\n",
    "    # Procesar todas las imágenes en el directorio\n",
    "    similarities = []\n",
    "    image_paths = list(Path(images_folder).glob(\"*.jpeg\")) + list(Path(images_folder).glob(\"*.jpg\")) + list(Path(images_folder).glob(\"*.png\"))\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                img_emb = clip_model.get_image_features(**inputs)\n",
    "                img_emb = img_emb / img_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            img_emb = img_emb.cpu().numpy()[0]\n",
    "\n",
    "            # Calcular similitud\n",
    "            similarity = np.dot(query_emb, img_emb)\n",
    "            similarities.append((str(img_path), similarity, img))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Ordenar por similitud (descendente)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Visualizar resultados\n",
    "    n_images = min(top_k + 1, len(similarities) + 1)\n",
    "\n",
    "    # Manejo especial si no hay suficientes imágenes\n",
    "    if n_images <= 1:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        if isinstance(input_image, np.ndarray):\n",
    "            ax.imshow(cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR))\n",
    "        else:\n",
    "            ax.imshow(Image.open(query_path))\n",
    "        ax.set_title(\"Query Image\")\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, n_images, figsize=(15, 4))\n",
    "\n",
    "        # Asegúrate de que axes es siempre una lista\n",
    "        if n_images == 2:  # Solo 2 imágenes (query + 1 similar)\n",
    "            axes = [axes[0], axes[1]]\n",
    "\n",
    "        # Imagen query\n",
    "        if isinstance(input_image, np.ndarray):\n",
    "            axes[0].imshow(input_image)  # Ya está en RGB si viene de img_rgb\n",
    "        else:\n",
    "            axes[0].imshow(Image.open(query_path))\n",
    "        axes[0].set_title(\"Query Image\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Imágenes similares\n",
    "        for i in range(min(top_k, len(similarities))):\n",
    "            path, score, img = similarities[i]\n",
    "            axes[i+1].imshow(img)\n",
    "            axes[i+1].set_title(f\"Score: {score:.4f}\")\n",
    "            axes[i+1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Limpiar archivo temporal si fue creado\n",
    "    if isinstance(input_image, np.ndarray) and os.path.exists(temp_path):\n",
    "        os.remove(temp_path)\n",
    "\n",
    "    # Devolver los resultados\n",
    "    return [(path, score) for path, score, _ in similarities[:top_k]]\n",
    "\n",
    "# Usar la función con la región recortada\n",
    "similar_images = rank_similar_images(crop, top_k=5)\n",
    "print(\"\\nImágenes más similares del manual:\")\n",
    "for i, (path, score) in enumerate(similar_images, 1):\n",
    "    print(f\"{i}. {os.path.basename(path)}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
