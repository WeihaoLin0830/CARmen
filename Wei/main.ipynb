{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51a58b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a90610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Carga de API keys desde .env ---\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "# --- Configuración de dispositivos ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Modelos para segmentación y caption ---\n",
    "predictor = SAM2ImagePredictor.from_pretrained(\n",
    "    \"facebook/sam2-hiera-large\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "processor_blip = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model_blip     = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\"\n",
    ").to(device)\n",
    "\n",
    "# --- Modelos para RAG+CLIP ---\n",
    "# CLIP para embeddings de crop\n",
    "clip_model     = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# MiniLM para embeddings de texto\n",
    "model_text     = SentenceTransformer(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Cliente y colecciones Chroma ---\n",
    "client    = chromadb.PersistentClient(path=\"chroma\")\n",
    "text_col  = client.get_or_create_collection(\n",
    "    name=\"tavascan_text\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "img_col   = client.get_or_create_collection(\n",
    "    name=\"tavascan_images\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# --- Lectura de imagen ---\n",
    "img_bgr = cv2.imread(\"C:/Users/weiha/Documents/HackUPC_2025/img/dashboard.jpeg\")\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9d6e39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordenadas en imagen original: (4,7) → (241,166)\n"
     ]
    }
   ],
   "source": [
    "# Variables globales\n",
    "ref_pt = []           # puntos de la selección (en coords de display)\n",
    "drawing = False       # flag de arrastre\n",
    "scale_percent = 100   # zoom inicial en porcentaje\n",
    "img = img_bgr.copy()  # imagen original\n",
    "h_orig, w_orig = img.shape[:2]\n",
    "\n",
    "# Función para recalcular la imagen mostrada según el zoom\n",
    "def update_display():\n",
    "    global img_display, img_copy, scale\n",
    "    scale = scale_percent / 100.0\n",
    "    w_disp = int(w_orig * scale)\n",
    "    h_disp = int(h_orig * scale)\n",
    "    img_display = cv2.resize(img, (w_disp, h_disp), interpolation=cv2.INTER_AREA)\n",
    "    img_copy = img_display.copy()\n",
    "    # Si ya hay ref_pt, redibuja el rectángulo en img_copy\n",
    "    if len(ref_pt) == 2:\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "\n",
    "# Callback para el trackbar de zoom\n",
    "def on_trackbar(val):\n",
    "    global scale_percent\n",
    "    scale_percent = max(val, 1)  # evita 0%\n",
    "    update_display()\n",
    "\n",
    "# Callback para el mouse: dibuja y captura la caja en coords de display\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global ref_pt, drawing, img_copy\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        ref_pt = [(x, y)]\n",
    "    elif event == cv2.EVENT_MOUSEMOVE and drawing:\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], (x, y), (0, 255, 0), 2)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        ref_pt.append((x, y))\n",
    "        img_copy = img_display.copy()\n",
    "        cv2.rectangle(img_copy, ref_pt[0], ref_pt[1], (0, 255, 0), 2)\n",
    "        # Mapea coords de display a coords originales\n",
    "        x0_disp, y0_disp = ref_pt[0]\n",
    "        x1_disp, y1_disp = ref_pt[1]\n",
    "        x0 = int(x0_disp / scale)\n",
    "        y0 = int(y0_disp / scale)\n",
    "        x1 = int(x1_disp / scale)\n",
    "        y1 = int(y1_disp / scale)\n",
    "        print(f\"Coordenadas en imagen original: ({x0},{y0}) → ({x1},{y1})\")\n",
    "\n",
    "# Prepara la ventana y los callbacks\n",
    "update_display()\n",
    "cv2.namedWindow(\"Selector de caja\")\n",
    "cv2.createTrackbar(\"Zoom %\", \"Selector de caja\", scale_percent, 300, on_trackbar)\n",
    "cv2.setMouseCallback(\"Selector de caja\", mouse_callback)\n",
    "\n",
    "# Bucle de visualización\n",
    "while True:\n",
    "    cv2.imshow(\"Selector de caja\", img_copy)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"r\"):       # 'r' resetea la selección\n",
    "        ref_pt = []\n",
    "        img_copy = img_display.copy()\n",
    "    elif key == ord(\"q\"):     # 'q' sale\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b362ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tras el bucle y antes de usar predictor:\n",
    "x0_disp, y0_disp = ref_pt[0]\n",
    "x1_disp, y1_disp = ref_pt[1]\n",
    "\n",
    "# Asegura que (x0_disp,y0_disp) sea la esquina superior-izquierda\n",
    "x0_disp, x1_disp = sorted([x0_disp, x1_disp])\n",
    "y0_disp, y1_disp = sorted([y0_disp, y1_disp])\n",
    "\n",
    "# Mapea a coordenadas de la imagen original y clampa\n",
    "x0 = int(x0_disp / scale)\n",
    "y0 = int(y0_disp / scale)\n",
    "x1 = int(x1_disp / scale)\n",
    "y1 = int(y1_disp / scale)\n",
    "\n",
    "# Evita salirse de los límites\n",
    "x0 = max(0, min(x0, w_orig - 1))\n",
    "y0 = max(0, min(y0, h_orig - 1))\n",
    "x1 = max(1, min(x1, w_orig    ))\n",
    "y1 = max(1, min(y1, h_orig    ))\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# boxleft = 492\n",
    "# boxtop = 116\n",
    "# boxwidth = 150\n",
    "# boxheight = 69\n",
    "\n",
    "# x0 = boxleft\n",
    "# y0 = boxtop\n",
    "# x1 = boxleft + boxwidth\n",
    "# y1 = boxtop + boxheight\n",
    "\n",
    "box = np.array([[x0, y0, x1, y1]], dtype=np.int32)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=box,\n",
    "        multimask_output=False,\n",
    "        return_logits=True\n",
    "    )\n",
    "\n",
    "mask = masks[0]\n",
    "crop = img_rgb[y0:y1, x0:x1]\n",
    "\n",
    "# convierte de RGB a BGR para cv2\n",
    "crop_bgr = cv2.cvtColor(crop, cv2.COLOR_RGB2BGR)\n",
    "cv2.imshow(\"Region seleccionada\", crop_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28d49e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined specific description: this part of the cupra tavascan dashboard is :'s first - gen's first - gen's first - gen's Based on the image, this component appears to be the **rearview mirror assembly**. It is centrally mounted at the top of the dashboard, featuring a dark-colored trapezoidal housing and a small rectangular sensor, likely for auto-dimming functionality.\n",
      "\n",
      "--- Available metadata example ---\n",
      "Available fields: ['start_page', 'section_title', 'section_id', 'chunk_index']\n",
      "Complete example: {'start_page': 227, 'section_title': 'Exit Warning', 'section_id': 'b6145e94-a5c1-43ae-8759-70b143fc79c4', 'chunk_index': 0}\n",
      "\n",
      "RAG Response with specific description:\n",
      "\n",
      "Based on the provided manual excerpts, the component in the image is the **automatic anti-dazzle interior mirror**.\n",
      "\n",
      "*   **Main Function:** To provide the driver with a clear view of the area behind the vehicle.\n",
      "*   **Secondary Function:** To automatically reduce glare from headlights of vehicles behind, preventing dazzling the driver. Light from screens of portable navigation devices can cause malfunctions of the automatic anti-dazzle interior mirror.\n",
      "*   **How it is used:** The manual mentions that abnormal operation of the automatic anti-dazzle function may result in it being impossible to use the interior mirror to precisely calculate the distance to vehicles driving behind, or to other objects.\n",
      "*   **Location:** The manual does not explicitly state the exact location of the interior mirror, but implies it is mounted centrally at the top of the dashboard.\n",
      "\n",
      "\n",
      "\n",
      "--- Fragments used (sorted by relevance) ---\n",
      "1. Text score: 0.6806\n",
      "   Text: Light from screens of portable navigation devices can cause malfunctions of the au- tomatic anti-daz...\n",
      "   Extracted page: p.126\n",
      "   Keywords found: control, screen, first, first, first, mirror\n",
      "   Available metadata:\n",
      "      - section_id: 9dea2f11-4c7c-4d9f-abd6-0b95fe30f4b9\n",
      "      - start_page: 126\n",
      "      - section_title: WARNING\n",
      "\n",
      "2. Text score: 0.6632\n",
      "   Text: Fig. 1  Digital Cockpit on the dash panel. The Digital Cockpit is a digital instrument clus- ter wit...\n",
      "   Extracted page: p.10\n",
      "   Keywords found: panel, instrument, digital, cockpit, display\n",
      "   Available metadata:\n",
      "      - section_title: Digital Cockpit\n",
      "      - start_page: 10\n",
      "      - section_id: 02b96b95-3e90-4fb1-aa50-cd12f3ed1594\n",
      "\n",
      "3. Text score: 0.5991\n",
      "   Text: Infotainment system Note As with most state-of-the-art computer and electronic equipment, in certain...\n",
      "   Extracted page: p.255\n",
      "   Keywords found: control, display, screen, appears\n",
      "   Available metadata:\n",
      "      - section_id: d4c2ed87-1145-4272-9fd0-0f8fc08e000d\n",
      "      - start_page: 255\n",
      "      - section_title: Infotainment system\n",
      "\n",
      "4. Text score: 0.5124\n",
      "   Text: Driver information Infotainment system operation and displays A B C D E F G Explanation of the funct...\n",
      "   Extracted page: p.26\n",
      "   Keywords found: control, display, screen\n",
      "   Available metadata:\n",
      "      - start_page: 26\n",
      "      - section_id: 38daaeec-4cc3-4c20-8f4f-face93e2eb8e\n",
      "      - section_title: Driver information\n",
      "\n",
      "5. Text score: 0.5100\n",
      "   Text: Infotainment system The other languages of the infotainment sys- tem (e.g. Greek, Russian, Turkish, ...\n",
      "   Extracted page: p.259\n",
      "   Keywords found: control, display, screen\n",
      "   Available metadata:\n",
      "      - section_title: Infotainment system\n",
      "      - start_page: 259\n",
      "      - section_id: 336e30ab-f817-4376-8f21-3c50117ba09e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- RAG+CLIP retrieval with enhanced specific description ---\n",
    "# 1) Embedding of the region with CLIP (for image collection)\n",
    "inputs_img = clip_processor(images=crop, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    img_emb = clip_model.get_image_features(**inputs_img)\n",
    "    img_emb = img_emb / img_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "img_emb = img_emb.cpu().numpy()[0]\n",
    "\n",
    "# 1b) Text embedding based on a specific and technical description of the image\n",
    "# Option 1: Improve BLIP with more specific conditioning text\n",
    "text = \"This part of the CUPRA Tavascan dashboard is:\"  # Specific conditioning text\n",
    "inputs_blip = processor_blip(\n",
    "    images=crop,\n",
    "    text=text,  # Add conditional text to guide the description\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "with torch.no_grad():\n",
    "    out = model_blip.generate(\n",
    "        **inputs_blip,\n",
    "        max_length=50,  # Allow longer descriptions\n",
    "        num_beams=5,    # Beam search for better quality\n",
    "        temperature=0.7 # Creativity control\n",
    "    )\n",
    "    blip_description = processor_blip.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "# Option 2: Use Gemini to get a more technical and precise description\n",
    "crop_pil = Image.fromarray(crop)\n",
    "description_prompt = \"\"\"\n",
    "Specifically describe this component of the CUPRA Tavascan dashboard.\n",
    "Include:\n",
    "- Exact technical name of the component\n",
    "- Its specific location on the dashboard\n",
    "- Distinctive visual characteristics (shape, color, symbols)\n",
    "- Type of control or indicator it appears to be\n",
    "Respond concisely in 1-2 technical sentences.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # If Gemini API key is available and working\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
    "    gemini_response = model.generate_content([description_prompt, crop_pil])\n",
    "    gemini_description = gemini_response.text.strip()\n",
    "\n",
    "    # Combine both descriptions for greater specificity\n",
    "    image_description = f\"{blip_description} {gemini_description}\"\n",
    "    print(f\"Combined specific description: {image_description}\")\n",
    "except Exception as e:\n",
    "    # Fallback to BLIP if Gemini fails\n",
    "    image_description = f\"{blip_description} Control or indicator on the CUPRA Tavascan dashboard.\"\n",
    "    print(f\"BLIP description (fallback): {image_description}\")\n",
    "\n",
    "# Generate text embedding to query text_col\n",
    "text_embedding = model_text.encode([image_description], normalize_embeddings=True)[0]\n",
    "\n",
    "# 2) Query to text collection with the enhanced description\n",
    "top_k = 5  # Total results to show\n",
    "\n",
    "# Query the text collection using the TEXT embedding\n",
    "txt_hits = text_col.query(\n",
    "    query_embeddings=[text_embedding.tolist()],\n",
    "    n_results=top_k*2,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "# 3) Process results using only text score\n",
    "combined_results = []\n",
    "\n",
    "# First, we inspect a result to see what fields the metadata actually has\n",
    "print(\"\\n--- Available metadata example ---\")\n",
    "if txt_hits[\"metadatas\"] and len(txt_hits[\"metadatas\"][0]) > 0:\n",
    "    sample_meta = txt_hits[\"metadatas\"][0][0]\n",
    "    print(f\"Available fields: {list(sample_meta.keys())}\")\n",
    "    print(f\"Complete example: {sample_meta}\")\n",
    "\n",
    "    # FUNCTION TO EXTRACT PAGE WITH DETAILED INSPECTION\n",
    "    def extract_page_info(metadata, text=None):\n",
    "        \"\"\"\n",
    "        Extracts page information with detailed inspection and text usage\n",
    "        Includes more potential sources of page information\n",
    "        \"\"\"\n",
    "        # 1. Look in common metadata fields\n",
    "        page_fields = [\"page\", \"start_page\", \"page_num\", \"page_number\", \"pagina\", \"page_id\"]\n",
    "        for field in page_fields:\n",
    "            if field in metadata and metadata[field]:\n",
    "                return f\"p.{metadata[field]}\"\n",
    "\n",
    "        # 2. Search all metadata fields for anything that looks like a page\n",
    "        for key, value in metadata.items():\n",
    "            if isinstance(value, str):\n",
    "                # Look for patterns like \"page 23\", \"p.45\", etc.\n",
    "                page_patterns = [\n",
    "                    r\"page\\s*(\\d+)\",\n",
    "                    r\"p\\.?\\s*(\\d+)\",\n",
    "                    r\"pagina\\s*(\\d+)\",\n",
    "                    r\"pág\\.?\\s*(\\d+)\"\n",
    "                ]\n",
    "                for pattern in page_patterns:\n",
    "                    match = re.search(pattern, value, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        return f\"p.{match.group(1)}\"\n",
    "\n",
    "        # 3. Look in file paths or sources\n",
    "        source_fields = [\"source\", \"path\", \"file\", \"filename\", \"filepath\"]\n",
    "        for field in source_fields:\n",
    "            if field in metadata and metadata[field]:\n",
    "                source_str = metadata[field]\n",
    "                match = re.search(r\"page[_-]?(\\d+)\", source_str, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return f\"p.{match.group(1)}\"\n",
    "\n",
    "        # 4. Try to extract page from text if available\n",
    "        if text:\n",
    "            # Look for page mentions in text\n",
    "            text_patterns = [\n",
    "                r\"page\\s*(\\d+)\",\n",
    "                r\"p\\.?\\s*(\\d+)\",\n",
    "                r\"page number[:\\s]*(\\d+)\",\n",
    "                r\"fig\\.?\\s+\\w+\\s+on\\s+page\\s+(\\d+)\"\n",
    "            ]\n",
    "            for pattern in text_patterns:\n",
    "                match = re.search(pattern, text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return f\"p.{match.group(1)}\"\n",
    "\n",
    "        return \"p.N/A\"  # If no page information is found\n",
    "\n",
    "    # Process results using only text score\n",
    "    if txt_hits[\"documents\"] and len(txt_hits[\"documents\"][0]) > 0:\n",
    "        for i, (doc, meta, distance) in enumerate(zip(\n",
    "            txt_hits[\"documents\"][0],\n",
    "            txt_hits[\"metadatas\"][0],\n",
    "            txt_hits[\"distances\"][0]\n",
    "        )):\n",
    "            # Convert distance to similarity (1 = very similar, 0 = not similar)\n",
    "            text_score = 1.0 - float(distance)\n",
    "\n",
    "            # Add relevance bonus if it contains important keywords\n",
    "            # based on the specific description\n",
    "            keywords = [\"indicator\", \"panel\", \"board\", \"dashboard\", \"control\", \"instrument\",\n",
    "                       \"digital\", \"cockpit\", \"display\", \"screen\"]\n",
    "\n",
    "            # Extract possible specific keywords from the description\n",
    "            specific_keywords = [word.lower() for word in image_description.split()\n",
    "                               if len(word) > 3 and word.lower() not in\n",
    "                               [\"this\", \"that\", \"cupra\", \"tavascan\", \"for\", \"like\", \"part\"]]\n",
    "\n",
    "            keywords.extend(specific_keywords)\n",
    "\n",
    "            # Calculate relevance boost based on keywords\n",
    "            relevance_boost = 0.0\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in doc.lower():\n",
    "                    relevance_boost += 0.05  # 5% boost for each keyword found\n",
    "\n",
    "            # Apply boost with a 20% maximum\n",
    "            boosted_score = min(text_score + relevance_boost, 1.0)\n",
    "\n",
    "            # Extract page information using the improved function\n",
    "            page_info = extract_page_info(meta, doc)\n",
    "\n",
    "            # Create result with boosted score\n",
    "            result = {\n",
    "                \"id\": f\"txt_{i}\",\n",
    "                \"text\": doc,\n",
    "                \"metadata\": meta,\n",
    "                \"text_score\": boosted_score,\n",
    "                \"page_info\": page_info,\n",
    "                \"combined_score\": boosted_score,\n",
    "                \"keywords_found\": [k for k in keywords if k.lower() in doc.lower()]\n",
    "            }\n",
    "\n",
    "            combined_results.append(result)\n",
    "\n",
    "# Sort by text score\n",
    "combined_results.sort(key=lambda x: x[\"text_score\"], reverse=True)\n",
    "combined_results = combined_results[:top_k]  # Keep only the best ones\n",
    "\n",
    "# 4) Build context with results\n",
    "context_items = []\n",
    "for i, result in enumerate(combined_results):\n",
    "    page_info = result[\"page_info\"]\n",
    "    score_info = f\"(relevance: {result['text_score']:.2f})\"\n",
    "\n",
    "    # Include found keywords for better context\n",
    "    keywords_info = \"\"\n",
    "    if result.get(\"keywords_found\"):\n",
    "        keywords_info = f\" [Keywords: {', '.join(result['keywords_found'][:3])}]\"\n",
    "\n",
    "    context_items.append(f\"- {result['text']} {page_info} {score_info}{keywords_info}\")\n",
    "\n",
    "context_txt = \"\\n\".join(context_items)\n",
    "\n",
    "# 5) Enhanced RAG prompt with specific description\n",
    "prompt = (\n",
    "    f\"You are an expert on the CUPRA Tavascan manual.\\n\"\n",
    "    f\"The image shows: {image_description}\\n\\n\"\n",
    "    f\"Relevant information from the manual (ordered by relevance):\\n{context_txt}\\n\\n\"\n",
    "    f\"What exactly is the component selected in the image? Describe its:\\n\"\n",
    "    f\"- Specific technical name\\n\"\n",
    "    f\"- Main and secondary function\\n\"\n",
    "    f\"- How it is used or interacted with\\n\"\n",
    "    f\"- Where it is located in the vehicle\\n\\n\"\n",
    "    f\"Answer with technical precision based solely on the information from the manual.\"\n",
    ")\n",
    "\n",
    "# 6) Gemini call including the image\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-001')\n",
    "crop_pil = Image.fromarray(crop)\n",
    "response = model.generate_content([prompt, crop_pil])\n",
    "print(\"\\nRAG Response with specific description:\\n\")\n",
    "print(response.text)\n",
    "\n",
    "# Show used fragments\n",
    "print(\"\\n--- Fragments used (sorted by relevance) ---\")\n",
    "for i, result in enumerate(combined_results):\n",
    "    print(f\"{i+1}. Text score: {result['text_score']:.4f}\")\n",
    "    print(f\"   Text: {result['text'][:100]}...\" if len(result['text']) > 100 else f\"   Text: {result['text']}\")\n",
    "    print(f\"   Extracted page: {result['page_info']}\")\n",
    "    if result.get(\"keywords_found\"):\n",
    "        print(f\"   Keywords found: {', '.join(result['keywords_found'])}\")\n",
    "\n",
    "    # Show detailed metadata information for debugging\n",
    "    print(\"   Available metadata:\")\n",
    "    for key, value in result[\"metadata\"].items():\n",
    "        if value:  # Only show fields with values\n",
    "            print(f\"      - {key}: {value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a7145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando imágenes similares a 'this part of the cupra tavascan dashboard is :'s first - gen's first - gen's first - gen's Based on the image, this component appears to be the **rearview mirror assembly**. It is centrally mounted at the top of the dashboard, featuring a dark-colored trapezoidal housing and a small rectangular sensor, likely for auto-dimming functionality.'\n",
      "Configuración: 0.60 similitud visual + 0.40 relevancia textual\n",
      "Calculando embedding visual con CLIP...\n",
      "Calculando embedding de texto con SentenceTransformer...\n",
      "Reutilizando resultados de texto de la consulta anterior...\n",
      "Caché de scores de texto disponible para 5 documentos\n",
      "Precalculando resultados de texto más similares...\n",
      "Analizando 221 imágenes...\n",
      "Procesado: 20/221 imágenes\n",
      "Procesado: 40/221 imágenes\n",
      "Procesado: 60/221 imágenes\n",
      "Procesado: 80/221 imágenes\n"
     ]
    }
   ],
   "source": [
    "# Directorio de imágenes del manual\n",
    "images_folder = \"C:/Users/weiha/Documents/HackUPC_2025/extracted_content_manual/images\"\n",
    "\n",
    "def rank_similar_images_combined(input_image, description, top_k=10, visual_weight=0.6, text_weight=0.4):\n",
    "    \"\"\"\n",
    "    Ranking de imágenes por similitud visual y textual combinadas con extracción mejorada de metadatos\n",
    "    aprovechando scores previos y optimizando consultas\n",
    "    \n",
    "    Args:\n",
    "        input_image: Imagen de consulta (numpy array)\n",
    "        description: Descripción textual de la imagen\n",
    "        top_k: Número de resultados a mostrar\n",
    "        visual_weight: Peso para la similitud visual (0-1)\n",
    "        text_weight: Peso para la similitud textual (0-1)\n",
    "    \"\"\"\n",
    "    # Declarar acceso a la variable global al inicio de la función\n",
    "    global combined_results\n",
    "\n",
    "    # Función mejorada para extraer página de cualquier fuente de información\n",
    "    def extract_page_info(text=None, metadata=None, filename=None):\n",
    "        \"\"\"Extrae número de página de múltiples fuentes posibles\"\"\"\n",
    "        # 1. Buscar en metadatos si están disponibles\n",
    "        if metadata:\n",
    "            # Campos comunes de página\n",
    "            page_fields = [\"page\", \"start_page\", \"page_num\", \"page_number\", \"pagina\", \"page_id\"]\n",
    "            for field in page_fields:\n",
    "                if field in metadata and metadata[field]:\n",
    "                    return f\"{metadata[field]}\"\n",
    "\n",
    "            # Buscar en cualquier campo de metadatos\n",
    "            for key, value in metadata.items():\n",
    "                if isinstance(value, str):\n",
    "                    # Patrones como \"page 23\", \"p.45\", etc.\n",
    "                    page_patterns = [\n",
    "                        r\"page\\s*(\\d+)\",\n",
    "                        r\"p\\.?\\s*(\\d+)\",\n",
    "                        r\"pagina\\s*(\\d+)\",\n",
    "                        r\"pág\\.?\\s*(\\d+)\"\n",
    "                    ]\n",
    "                    for pattern in page_patterns:\n",
    "                        match = re.search(pattern, value, re.IGNORECASE)\n",
    "                        if match:\n",
    "                            return match.group(1)\n",
    "\n",
    "        # 2. Buscar en el nombre de archivo\n",
    "        if filename:\n",
    "            # Patrones comunes en nombres de archivo\n",
    "            page_patterns = [\n",
    "                r\"page[_-]?(\\d+)\",\n",
    "                r\"p(\\d+)\",\n",
    "                r\"_(\\d+)\\.(?:jpe?g|png)$\"  # Números antes de la extensión\n",
    "            ]\n",
    "            for pattern in page_patterns:\n",
    "                match = re.search(pattern, filename, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "\n",
    "        # 3. Buscar en el texto si está disponible\n",
    "        if text:\n",
    "            text_patterns = [\n",
    "                r\"page\\s*(\\d+)\",\n",
    "                r\"p\\.?\\s*(\\d+)\",\n",
    "                r\"page number[:\\s]*(\\d+)\",\n",
    "                r\"fig\\.?\\s+\\w+\\s+on\\s+page\\s+(\\d+)\"\n",
    "            ]\n",
    "            for pattern in text_patterns:\n",
    "                match = re.search(pattern, text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    return match.group(1)\n",
    "\n",
    "        return \"N/A\"\n",
    "\n",
    "    # Normalizar pesos para que sumen 1\n",
    "    total = visual_weight + text_weight\n",
    "    visual_weight = visual_weight / total\n",
    "    text_weight = text_weight / total\n",
    "\n",
    "    print(f\"Buscando imágenes similares a '{description}'\")\n",
    "    print(f\"Configuración: {visual_weight:.2f} similitud visual + {text_weight:.2f} relevancia textual\")\n",
    "\n",
    "    # 1. Obtener embedding visual con CLIP\n",
    "    print(\"Calculando embedding visual con CLIP...\")\n",
    "    inputs_img = clip_processor(images=input_image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        visual_emb = clip_model.get_image_features(**inputs_img)\n",
    "        visual_emb = visual_emb / visual_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    visual_emb = visual_emb.cpu().numpy()[0]\n",
    "\n",
    "    # 2. Obtener embedding textual con SentenceTransformer\n",
    "    print(\"Calculando embedding de texto con SentenceTransformer...\")\n",
    "    text_emb = model_text.encode([description], normalize_embeddings=True)[0]\n",
    "\n",
    "    # 3. OPTIMIZACIÓN: Reutilizar resultados de texto si ya existen en el contexto global\n",
    "    text_scores_cache = {}\n",
    "    if 'combined_results' in globals() and combined_results is not None and len(combined_results) > 0:\n",
    "        print(\"Reutilizando resultados de texto de la consulta anterior...\")\n",
    "\n",
    "        # Extraer scores y metadatos de resultados previos\n",
    "        for result in combined_results:\n",
    "            if 'text_score' in result and 'metadata' in result and 'text' in result:\n",
    "                key = result.get('text', '')[:50]  # Usar inicio del texto como clave\n",
    "                if key:\n",
    "                    text_scores_cache[key] = {\n",
    "                        'score': result['text_score'],\n",
    "                        'metadata': result['metadata'],\n",
    "                        'text': result['text']\n",
    "                    }\n",
    "\n",
    "        print(f\"Caché de scores de texto disponible para {len(text_scores_cache)} documentos\")\n",
    "    else:\n",
    "        text_scores_cache = {}\n",
    "\n",
    "    # 4. Obtener textos relacionados solo si no hay caché suficiente\n",
    "    if len(text_scores_cache) < 30:  # Si tenemos menos de 30 resultados en caché, consultar\n",
    "        print(\"Precalculando resultados de texto más similares...\")\n",
    "        general_text_matches = text_col.query(\n",
    "            query_embeddings=[text_emb.tolist()],\n",
    "            n_results=50,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "\n",
    "        # Crear un mapa de página -> mejores resultados para búsquedas rápidas\n",
    "        page_to_text_results = {}\n",
    "        if general_text_matches[\"documents\"] and len(general_text_matches[\"documents\"][0]) > 0:\n",
    "            for i, (doc, meta, distance) in enumerate(zip(\n",
    "                general_text_matches[\"documents\"][0],\n",
    "                general_text_matches[\"metadatas\"][0],\n",
    "                general_text_matches[\"distances\"][0]\n",
    "            )):\n",
    "                text_score = 1.0 - float(distance)\n",
    "\n",
    "                # Añadir a la caché para futuras consultas\n",
    "                key = doc[:50]\n",
    "                text_scores_cache[key] = {\n",
    "                    'score': text_score,\n",
    "                    'metadata': meta,\n",
    "                    'text': doc\n",
    "                }\n",
    "\n",
    "                # Extraer página del texto/metadatos\n",
    "                extracted_page = extract_page_info(text=doc, metadata=meta)\n",
    "\n",
    "                if extracted_page != \"N/A\":\n",
    "                    if extracted_page not in page_to_text_results:\n",
    "                        page_to_text_results[extracted_page] = []\n",
    "\n",
    "                    page_to_text_results[extracted_page].append({\n",
    "                        \"doc\": doc,\n",
    "                        \"meta\": meta,\n",
    "                        \"score\": text_score\n",
    "                    })\n",
    "    else:\n",
    "        # Usar la caché para crear el mapeo de página a resultados\n",
    "        page_to_text_results = {}\n",
    "        for key, item in text_scores_cache.items():\n",
    "            doc = item['text']\n",
    "            meta = item['metadata']\n",
    "            text_score = item['score']\n",
    "\n",
    "            extracted_page = extract_page_info(text=doc, metadata=meta)\n",
    "            if extracted_page != \"N/A\":\n",
    "                if extracted_page not in page_to_text_results:\n",
    "                    page_to_text_results[extracted_page] = []\n",
    "\n",
    "                page_to_text_results[extracted_page].append({\n",
    "                    \"doc\": doc,\n",
    "                    \"meta\": meta,\n",
    "                    \"score\": text_score\n",
    "                })\n",
    "\n",
    "    # 5. Buscar similitudes visuales y textuales para cada imagen\n",
    "    image_paths = list(Path(images_folder).glob(\"*.jpeg\")) + \\\n",
    "                  list(Path(images_folder).glob(\"*.jpg\")) + \\\n",
    "                  list(Path(images_folder).glob(\"*.png\"))\n",
    "\n",
    "    # Asegurar que hay imágenes para procesar\n",
    "    if not image_paths:\n",
    "        print(f\"No se encontraron imágenes en {images_folder}\")\n",
    "        return []\n",
    "\n",
    "    # OPTIMIZACIÓN: Caché para similitudes visuales\n",
    "    visual_sim_cache = {}\n",
    "\n",
    "    print(f\"Analizando {len(image_paths)} imágenes...\")\n",
    "\n",
    "    # 6. Procesar cada imagen y calcular scores combinados\n",
    "    combined_scores = []\n",
    "    processed_count = 0\n",
    "\n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            img_name = os.path.basename(img_path)\n",
    "            processed_count += 1\n",
    "            if processed_count % 20 == 0:\n",
    "                print(f\"Procesado: {processed_count}/{len(image_paths)} imágenes\")\n",
    "\n",
    "            # 6.1 Similitud visual con CLIP\n",
    "            # Comprobar si ya tenemos esta similitud en caché\n",
    "            if str(img_path) in visual_sim_cache:\n",
    "                visual_similarity = visual_sim_cache[str(img_path)]\n",
    "            else:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                inputs = clip_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    img_emb = clip_model.get_image_features(**inputs)\n",
    "                    img_emb = img_emb / img_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "                img_emb_np = img_emb.cpu().numpy()[0]\n",
    "\n",
    "                # Calcular similitud visual (producto punto)\n",
    "                visual_similarity = float(np.dot(visual_emb, img_emb_np))\n",
    "\n",
    "                # Guardar en caché para futuras consultas\n",
    "                visual_sim_cache[str(img_path)] = visual_similarity\n",
    "\n",
    "            # 6.2 Extraer información y número de página del nombre del archivo\n",
    "            page_num = extract_page_info(filename=img_name)\n",
    "\n",
    "            # 6.3 Similitud textual - buscar texto relacionado usando nuestros resultados precalculados\n",
    "            text_similarity = 0.0\n",
    "            metadata_info = {}\n",
    "            related_text = \"\"\n",
    "\n",
    "            if page_num != \"N/A\" and page_num in page_to_text_results:\n",
    "                # Usar la mejor similitud de texto para esta página\n",
    "                best_match = max(page_to_text_results[page_num], key=lambda x: x[\"score\"])\n",
    "                text_similarity = best_match[\"score\"]\n",
    "                metadata_info = best_match[\"meta\"]\n",
    "                related_text = best_match[\"doc\"]\n",
    "\n",
    "                # Aplicar boost de relevancia por keyword matching\n",
    "                keywords = [\"indicador\", \"panel\", \"tablero\", \"dashboard\", \"control\", \"instrumento\",\n",
    "                           \"digital\", \"cockpit\", \"display\", \"pantalla\"]\n",
    "\n",
    "                # Extraer palabras clave específicas de la descripción\n",
    "                specific_keywords = [word.lower() for word in description.split()\n",
    "                                   if len(word) > 3 and word.lower() not in\n",
    "                                   [\"esta\", \"este\", \"cupra\", \"tavascan\", \"para\", \"como\", \"part\"]]\n",
    "\n",
    "                keywords.extend(specific_keywords)\n",
    "\n",
    "                # Calcular bonus por palabras clave\n",
    "                relevance_boost = 0.0\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in related_text.lower():\n",
    "                        relevance_boost += 0.02  # 2% boost por palabra clave\n",
    "\n",
    "                # Aplicar boost con máximo de 15%\n",
    "                text_similarity = min(text_similarity + relevance_boost, 1.0)\n",
    "            else:\n",
    "                # Si no hay match exacto por página, usar texto general basado en similitud con la descripción\n",
    "                best_score = 0.0\n",
    "                for key, item in text_scores_cache.items():\n",
    "                    if item['score'] > best_score:\n",
    "                        best_score = item['score']\n",
    "                        metadata_info = item['metadata']\n",
    "                        related_text = item['text']\n",
    "\n",
    "                if best_score > 0:\n",
    "                    text_similarity = best_score\n",
    "\n",
    "                    # Si no teníamos página, intentar extraerla del texto\n",
    "                    if page_num == \"N/A\":\n",
    "                        page_from_text = extract_page_info(text=related_text, metadata=metadata_info)\n",
    "                        if page_from_text != \"N/A\":\n",
    "                            page_num = page_from_text\n",
    "\n",
    "            # 6.4 Calcular score combinado\n",
    "            combined_score = (visual_weight * visual_similarity) + (text_weight * text_similarity)\n",
    "\n",
    "            # 6.5 Añadir a resultados\n",
    "            combined_scores.append({\n",
    "                \"path\": str(img_path),\n",
    "                \"image\": img,\n",
    "                \"visual_score\": float(visual_similarity),\n",
    "                \"text_score\": float(text_similarity),\n",
    "                \"combined_score\": float(combined_score),\n",
    "                \"metadata\": metadata_info,\n",
    "                \"filename\": img_name,\n",
    "                \"page\": page_num,\n",
    "                \"related_text\": related_text[:100] + \"...\" if len(related_text) > 100 else related_text\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {img_path}: {e}\")\n",
    "\n",
    "    # 7. Ordenar por puntuación combinada\n",
    "    combined_scores.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    top_results = combined_scores[:top_k]\n",
    "\n",
    "    # 8. Visualizar resultados\n",
    "    n_cols = 5\n",
    "    n_rows = (min(top_k, len(combined_scores)) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if top_k == 1 else axes\n",
    "\n",
    "    # Mostrar la imagen de consulta en la primera posición\n",
    "    axes[0].imshow(input_image)\n",
    "    axes[0].set_title(\"Imagen de consulta\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Mostrar el resto de resultados\n",
    "    for i, result in enumerate(top_results):\n",
    "        if i >= len(axes) - 1:  # -1 porque la primera es la consulta\n",
    "            break\n",
    "\n",
    "        axes[i+1].imshow(result[\"image\"])\n",
    "        axes[i+1].set_title(f\"#{i+1}: {result['combined_score']:.4f}\\n\" +\n",
    "                           f\"(V:{result['visual_score']:.2f}, T:{result['text_score']:.2f})\\nPág: {result['page']}\")\n",
    "        axes[i+1].axis(\"off\")\n",
    "\n",
    "    # Ocultar ejes vacíos\n",
    "    for i in range(len(top_results) + 1, len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 9. Mostrar tabla de resultados detallada\n",
    "    print(\"\\n====== TOP 10 IMÁGENES MÁS SIMILARES ======\")\n",
    "    print(f\"{'#':<3} {'SCORE':<10} {'VISUAL':<10} {'TEXTO':<10} {'PÁGINA':<8} {'ARCHIVO'}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    for i, result in enumerate(top_results):\n",
    "        print(f\"{i+1:<3} {result['combined_score']:.6f} {result['visual_score']:.6f} \"\n",
    "             f\"{result['text_score']:.6f} {result['page']:<8} {result['filename']}\")\n",
    "\n",
    "        # Mostrar texto relacionado si existe\n",
    "        if result[\"related_text\"]:\n",
    "            print(f\"    Texto relacionado: {result['related_text']}\")\n",
    "\n",
    "    # Guardar los resultados en una variable global para reutilización\n",
    "    combined_results = top_results\n",
    "\n",
    "    return top_results\n",
    "\n",
    "# Llamar a la función de ranking de imágenes similares\n",
    "rank_similar_images_combined(crop, image_description, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
