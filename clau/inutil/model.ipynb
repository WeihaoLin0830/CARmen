{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f1f90b9",
   "metadata": {},
   "source": [
    "modelo 1\n",
    "\n",
    "funciona correcto clip -> mejorar descripciones con indice\n",
    "\n",
    "vit -> descripciones no encajan\n",
    "\n",
    "ocr -> lo hace correcto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import easyocr\n",
    "\n",
    "# ---------- CONFIGURACI√ìN ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_path = \"porta_esquerra.jpeg\"  # Cambia aqu√≠ tu imagen\n",
    "\n",
    "# ---------- 1. CLIP ----------\n",
    "print(\"\\nüß† CLIP: comparando con descripciones conocidas...\")\n",
    "\n",
    "import open_clip  # Aseg√∫rate de haber hecho: pip install open_clip_torch\n",
    "\n",
    "# Cargar modelo CLIP desde open_clip\n",
    "model_clip, _, preprocess_clip = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "model_clip.to(device)\n",
    "\n",
    "image_clip = preprocess_clip(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "descriptions = [\n",
    "    \"Pantalla central del sistema de infoentretenimiento\",\n",
    "    \"Volante con botones multifunci√≥n\",\n",
    "    \"Guantera del lado del copiloto\",\n",
    "    \"Puerta derecha delantera\",\n",
    "    \"Zona trasera con asiento y cintur√≥n\",\n",
    "    \"Bot√≥n de arranque del veh√≠culo\",\n",
    "    \"Panel de control de climatizaci√≥n\",\n",
    "]\n",
    "\n",
    "text_tokens = tokenizer(descriptions).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model_clip.encode_image(image_clip)\n",
    "    text_features = model_clip.encode_text(text_tokens)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarities = (image_features @ text_features.T).squeeze(0)\n",
    "    best_idx = similarities.argmax().item()\n",
    "\n",
    "print(f\"üîç CLIP sugiere: '{descriptions[best_idx]}'\")\n",
    "for i, sim in enumerate(similarities):\n",
    "    print(f\"   - {descriptions[i]:<50} ‚Üí score: {sim:.3f}\")\n",
    "\n",
    "# ---------- 2. ViT-GPT2 (en lugar de BLIP-2) ----------\n",
    "print(\"\\nüßæ ViT-GPT2: generando descripci√≥n autom√°tica...\")\n",
    "\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "model_blip = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model_blip.generate(pixel_values, max_length=50, num_beams=4)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(f\"üí¨ Descripci√≥n generada por ViT-GPT2: {caption}\")\n",
    "\n",
    "# ---------- 3. OCR (EasyOCR) ----------\n",
    "print(\"\\nüî† OCR: texto detectado en la imagen...\")\n",
    "reader = easyocr.Reader(['en', 'es'], gpu=torch.cuda.is_available())\n",
    "results = reader.readtext(image_path)\n",
    "\n",
    "if not results:\n",
    "    print(\"   (No se detect√≥ texto visible)\")\n",
    "else:\n",
    "    for (bbox, text, prob) in results:\n",
    "        print(f\"   - '{text}' (confianza: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37900797",
   "metadata": {},
   "source": [
    "gitbase -> 1 bien 1 mal, muy especifico, escaso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "\n",
    "# ---------- CONFIGURACI√ìN ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_path = \"porta_esquerra.jpeg\"  # Cambia aqu√≠ tu imagen\n",
    "\n",
    "# ---------- GIT base: Generar descripci√≥n ----------\n",
    "print(\"\\nüßæ GIT base: generando descripci√≥n autom√°tica...\")\n",
    "\n",
    "# Cargar el modelo y el procesador\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\").to(device)\n",
    "\n",
    "# Preprocesar la imagen\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_length=50)\n",
    "    caption = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"üí¨ Descripci√≥n generada por GIT base: {caption}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe9ea6",
   "metadata": {},
   "source": [
    "blipbase -> no acaba de generar buenas descripciones, se nota que describe coche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57231879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# ---------- CONFIGURACI√ìN ----------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_path = \"porta_esquerra.jpeg\"  # Cambia aqu√≠ tu imagen\n",
    "\n",
    "# ---------- BLIP base: Generar descripci√≥n ----------\n",
    "print(\"\\nüßæ BLIP (base): generando descripci√≥n autom√°tica...\")\n",
    "\n",
    "# Cargar modelo y procesador\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# Cargar y procesar imagen\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generar caption\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "# Mostrar resultado\n",
    "print(f\"üí¨ Descripci√≥n generada por BLIP base: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cbaabe",
   "metadata": {},
   "source": [
    "ofa -> no llego ha ejecutarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157877e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
